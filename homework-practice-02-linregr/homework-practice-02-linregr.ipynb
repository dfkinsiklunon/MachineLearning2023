{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2 - предобработка признаков, pandas\n",
    "\n",
    "\n",
    "### О задании\n",
    "\n",
    "Практическое задание 2 посвящено изучению основных библиотек для анализа данных, а также линейных моделей и методов их обучения. Вы научитесь:\n",
    " * применять библиотеки NumPy и Pandas для осуществления желаемых преобразований;\n",
    " * подготавливать данные для обучения линейных моделей;\n",
    " * обучать линейную, Lasso и Ridge-регрессии при помощи модуля scikit-learn;\n",
    " * реализовывать обычный и стохастический градиентные спуски;\n",
    " * обучать линейную регрессию для произвольного функционала качества.\n",
    " \n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов. Кроме того, некоторые из заданий являются опциональными (необязательными), однако за их выполнение можно получить дополнительные баллы, которые позднее будут учитываться при проставлении оценок автоматом по курсу.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце Вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник). \n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "\n",
    "\n",
    "### Формат сдачи\n",
    "Для сдачи задания получившийся файл \\*.ipynb с решением необходимо выложить в свой репозиторий github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотеки для анализа данных\n",
    "\n",
    "### NumPy\n",
    "\n",
    "Во всех заданиях данного раздела запрещено использовать циклы  и list comprehensions. Под вектором и матрицей в данных заданиях понимается одномерный и двумерный numpy.array соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. (0.2 балла)** Реализуйте функцию, возвращающую максимальный элемент в векторе x среди элементов, перед которыми стоит нулевой. Для x = np.array([6, 2, 0, 3, 0, 0, 5, 7, 0]) ответом является 5. Если нулевых элементов нет, функция должна возвращать None.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "def max_after_zero(x):\n",
    "    # Find the indices of zero elements in the array\n",
    "    zero_indices = np.where(x == 0)[0]\n",
    "\n",
    "    # Shift the indices one position to the right to get the indices of elements after zero\n",
    "    next_indices = zero_indices + 1\n",
    "\n",
    "    # Filter out indices that are out of bounds\n",
    "    valid_indices = next_indices[next_indices < len(x)]\n",
    "\n",
    "    # Extract the elements after zero using the valid indices\n",
    "    elements_after_zero = x[valid_indices]\n",
    "\n",
    "    # Return the maximum element among the elements after zero\n",
    "    if len(elements_after_zero) > 0:\n",
    "        return np.max(elements_after_zero)\n",
    "    else:\n",
    "        return None  # Return None if there are no elements after zero\n",
    "\n",
    "# Test the function\n",
    "x = np.array([6, 2, 0, 3, 0, 0, 5, 7, 0])\n",
    "result = max_after_zero(x)\n",
    "print(result) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. (0.2 балла)** Реализуйте функцию, принимающую на вход матрицу и некоторое число и возвращающую ближайший к числу элемент матрицы. Например: для X = np.arange(0,10).reshape((2, 5)) и v = 3.6 ответом будет 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "def closest_element(matrix, number):\n",
    "    # Calculate the absolute difference between each element and the target number\n",
    "    absolute_diff = np.abs(matrix - number)\n",
    "    \n",
    "    # Find the indices of the element with the minimum absolute difference\n",
    "    min_index = np.unravel_index(np.argmin(absolute_diff), matrix.shape)\n",
    "    \n",
    "    # Get the element at the found index\n",
    "    closest = matrix[min_index]\n",
    "    \n",
    "    return closest\n",
    "\n",
    "# Example usage:\n",
    "X = np.arange(0, 10).reshape((2, 5))\n",
    "v = 3.6\n",
    "result = closest_element(X, v)\n",
    "print(result) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. (0.2 балла)** Реализуйте функцию scale(X), которая принимает на вход матрицу и масштабирует каждый ее столбец (вычитает выборочное среднее и делит на стандартное отклонение). Убедитесь, что в функции не будет происходить деления на ноль. Протестируйте на случайной матрице (для её генерации можно использовать, например, функцию [numpy.random.randint](http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randint.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Matrix:\n",
      "[[8 3 6]\n",
      " [3 2 9]\n",
      " [5 8 3]\n",
      " [1 8 7]\n",
      " [6 9 8]]\n",
      "\n",
      "Scaled Matrix:\n",
      "[[ 1.40693001 -1.03509834 -0.29138576]\n",
      " [-0.66208471 -1.38013112  1.16554303]\n",
      " [ 0.16552118  0.69006556 -1.74831455]\n",
      " [-1.4896906   0.69006556  0.19425717]\n",
      " [ 0.57932412  1.03509834  0.6799001 ]]\n"
     ]
    }
   ],
   "source": [
    "def scale(X):\n",
    "    # Calculate the mean and standard deviation of each column\n",
    "    column_means = np.mean(X, axis=0)\n",
    "    column_stds = np.std(X, axis=0)\n",
    "    \n",
    "    # Ensure there's no division by zero, replace zeros with 1\n",
    "    column_stds[column_stds == 0] = 1.0\n",
    "    \n",
    "    # Scale each column\n",
    "    scaled_X = (X - column_means) / column_stds\n",
    "    \n",
    "    return scaled_X\n",
    "\n",
    "# Generate a random matrix for testing\n",
    "random_matrix = np.random.randint(1, 10, size=(5, 3))\n",
    "\n",
    "# Scale the random matrix\n",
    "scaled_matrix = scale(random_matrix)\n",
    "\n",
    "# Display the scaled matrix\n",
    "print(\"Original Matrix:\")\n",
    "print(random_matrix)\n",
    "print(\"\\nScaled Matrix:\")\n",
    "print(scaled_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. (0.2 балла)** Реализуйте функцию, которая для заданной матрицы находит:\n",
    " - определитель\n",
    " - след\n",
    " - наименьший и наибольший элементы\n",
    " - норму Фробениуса\n",
    " - собственные числа\n",
    " - обратную матрицу\n",
    "\n",
    "Для тестирования сгенерируйте матрицу с элементами из нормального распределения $\\mathcal{N}$(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant: -102.93816078358077\n",
      "Trace: 36.20154411013948\n",
      "Smallest Element: 6.977278846911732\n",
      "Largest Element: 11.716428226745606\n",
      "Frobenius Norm: 38.54750733616194\n",
      "Eigenvalues: [38.12843156+0.j         -2.11364351+0.j          0.09337803+1.12631648j\n",
      "  0.09337803-1.12631648j]\n",
      "Inverse Matrix: [[ 0.29619032 -0.31318854  0.52446614 -0.3992702 ]\n",
      " [ 0.49155983 -0.42310666  0.16462284 -0.12097447]\n",
      " [-0.55793425  0.01948319 -0.09766659  0.61333581]\n",
      " [-0.14564023  0.68318274 -0.54313411 -0.07609596]]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(X):\n",
    "    # Your code here\n",
    "    import numpy as np\n",
    "\n",
    "def matrix_properties(matrix):\n",
    "    # Determinant\n",
    "    det = np.linalg.det(matrix)\n",
    "    \n",
    "    # Trace\n",
    "    trace = np.trace(matrix)\n",
    "    \n",
    "    # Smallest and largest elements\n",
    "    smallest = np.min(matrix)\n",
    "    largest = np.max(matrix)\n",
    "    \n",
    "    # Frobenius norm\n",
    "    frobenius_norm = np.linalg.norm(matrix, 'fro')\n",
    "    \n",
    "    # Eigenvalues\n",
    "    eigenvalues = np.linalg.eigvals(matrix)\n",
    "    \n",
    "    # Inverse matrix\n",
    "    try:\n",
    "        inverse_matrix = np.linalg.inv(matrix)\n",
    "    except np.linalg.LinAlgError:\n",
    "        inverse_matrix = None\n",
    "    \n",
    "    return {\n",
    "        \"Determinant\": det,\n",
    "        \"Trace\": trace,\n",
    "        \"Smallest Element\": smallest,\n",
    "        \"Largest Element\": largest,\n",
    "        \"Frobenius Norm\": frobenius_norm,\n",
    "        \"Eigenvalues\": eigenvalues,\n",
    "        \"Inverse Matrix\": inverse_matrix\n",
    "    }\n",
    "\n",
    "# Generate a random matrix with elements from a normal distribution\n",
    "random_matrix = np.random.normal(10, 1, size=(4, 4))\n",
    "\n",
    "# Calculate matrix properties\n",
    "result = matrix_properties(random_matrix)\n",
    "\n",
    "# Display the results\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. (0.2 балла)** Повторите 100 раз следующий эксперимент: сгенерируйте две матрицы размера 10×10 из стандартного нормального распределения, перемножьте их (как матрицы) и найдите максимальный элемент. Какое среднее значение по экспериментам у максимальных элементов? 95-процентная квантиль?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Maximum Element: 8.227937605160266\n",
      "95th Percentile Maximum Element: 11.260147599088283\n"
     ]
    }
   ],
   "source": [
    "for exp_num in range(100):\n",
    "    # Your code here\n",
    "    import numpy as np\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Number of experiments\n",
    "num_experiments = 100\n",
    "\n",
    "# Initialize an array to store the maximum elements from each experiment\n",
    "max_elements = np.empty(num_experiments)\n",
    "\n",
    "# Perform the experiment 100 times\n",
    "for i in range(num_experiments):\n",
    "    # Generate two 10x10 matrices from a standard normal distribution\n",
    "    matrix1 = np.random.normal(0, 1, size=(10, 10))\n",
    "    matrix2 = np.random.normal(0, 1, size=(10, 10))\n",
    "    \n",
    "    # Multiply the matrices\n",
    "    product_matrix = np.dot(matrix1, matrix2)\n",
    "    \n",
    "    # Find the maximum element in the product matrix\n",
    "    max_element = np.max(product_matrix)\n",
    "    \n",
    "    # Store the maximum element\n",
    "    max_elements[i] = max_element\n",
    "\n",
    "# Calculate the average and 95th percentile of the maximum elements\n",
    "average_max = np.mean(max_elements)\n",
    "quantile_95 = np.percentile(max_elements, 95)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Average Maximum Element: {average_max}\")\n",
    "print(f\"95th Percentile Maximum Element: {quantile_95}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Maximum Element: 8.302550555887155\n"
     ]
    }
   ],
   "source": [
    "# Set the seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Number of experiments\n",
    "num_experiments = 100\n",
    "\n",
    "# Generate two 10x10 matrices from a standard normal distribution for all experiments\n",
    "matrix1 = np.random.normal(0, 1, size=(num_experiments, 10, 10))\n",
    "matrix2 = np.random.normal(0, 1, size=(num_experiments, 10, 10))\n",
    "\n",
    "# Multiply the matrices element-wise\n",
    "product_matrices = np.matmul(matrix1, matrix2)\n",
    "\n",
    "# Find the maximum element in each product matrix\n",
    "max_elements = np.max(product_matrices, axis=(1, 2))\n",
    "\n",
    "# Calculate the average maximum element\n",
    "average_max = np.mean(max_elements)\n",
    "\n",
    "# Display the average maximum element\n",
    "print(f\"Average Maximum Element: {average_max}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "\n",
    "![](https://metrouk2.files.wordpress.com/2015/10/panda.jpg)\n",
    "\n",
    "#### Ответьте на вопросы о данных по авиарейсам в США за январь-апрель 2008 года.\n",
    "\n",
    "Данные находятся в приложенном файле `2008.csv`. Их [описание](http://stat-computing.org/dataexpo/2009/the-data.html) приведено ниже:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airline on-time performance\n",
    "\n",
    "Have you ever been stuck in an airport because your flight was delayed or cancelled and wondered if you could have predicted it if you'd had more data? This is your chance to find out.\n",
    "\n",
    "The data\n",
    "The data set is available for download here.\n",
    "The data consists of flight arrival and departure details for all commercial flights within the USA, from October 1987 to April 2008. This is a large dataset: there are nearly 120 million records in total, and takes up 1.6 gigabytes of space compressed and 12 gigabytes when uncompressed.\n",
    "\n",
    "Understanding and preparing the data\n",
    "In order to answer above questions, we are going to analyze the provided dataset, containing up to 1936758 ### different internal flights in the US for 2008 and their causes for delay, diversion and cancellation\n",
    "\n",
    "The data comes from the U.S. Department of Transportation’s (DOT) Bureau of Transportation Statistics (BTS). Meta data explanations\n",
    "\n",
    "This dataset is composed by the following variables:\n",
    "\n",
    "**Year** 2008 **Month** 1-12 **DayofMonth** 1-31 **DayOfWeek** 1 (Monday) - 7 (Sunday)  \n",
    "**DepTime** actual departure time (local, hhmm)  \n",
    "**CRSDepTime** scheduled departure time (local, hhmm)  \n",
    "**ArrTime** actual arrival time (local, hhmm)  \n",
    "**CRSArrTime** scheduled arrival time (local, hhmm)  \n",
    "**UniqueCarrier** unique carrier code  \n",
    "**FlightNum** flight number  \n",
    "**TailNum** plane tail number: aircraft registration, unique aircraft identifier  \n",
    "**ActualElapsedTime** in minutes  \n",
    "**CRSElapsedTime** in minutes  \n",
    "**AirTime** in minutes  \n",
    "**ArrDelay** arrival delay, in minutes: A flight is counted as “on time” if it operated less than 15 minutes later the scheduled time shown in the carriers’ Computerized Reservations Systems (CRS).  \n",
    "**DepDelay** departure delay, in minutes  \n",
    "**Origin** origin IATA airport code  \n",
    "**Dest** destination IATA airport code  \n",
    "**Distance** in miles  \n",
    "**TaxiIn** taxi in time, in minutes  \n",
    "**TaxiOut** taxi out time in minutes  \n",
    "**Cancelled** *was the flight cancelled  \n",
    "**CancellationCode** reason for cancellation (A = carrier, B = weather, C = NAS, D = security)  \n",
    "**Diverted** 1 = yes, 0 = no  \n",
    "**CarrierDelay** in minutes: Carrier delay is within the control of the air carrier. Examples of occurrences that may determine carrier delay are: aircraft cleaning, aircraft damage, awaiting the arrival of connecting passengers or crew, baggage, bird strike, cargo loading, catering, computer, outage-carrier equipment, crew legality (pilot or attendant rest), damage by hazardous goods, engineering inspection, fueling, handling disabled passengers, late crew, lavatory servicing, maintenance, oversales, potable water servicing, removal of unruly passenger, slow boarding or seating, stowing carry-on baggage, weight and balance delays.  \n",
    "**WeatherDelay** in minutes: Weather delay is caused by extreme or hazardous weather conditions that are forecasted or manifest themselves on point of departure, enroute, or on point of arrival.  \n",
    "**NASDelay** in minutes: Delay that is within the control of the National Airspace System (NAS) may include: non-extreme weather conditions, airport operations, heavy traffic volume, air traffic control, etc.  \n",
    "**SecurityDelay** in minutes: Security delay is caused by evacuation of a terminal or concourse, re-boarding of aircraft because of security breach, inoperative screening equipment and/or long lines in excess of 29 minutes at screening areas.  \n",
    "**LateAircraftDelay** in minutes: Arrival delay at an airport due to the late arrival of the same aircraft at a previous airport. The ripple effect of an earlier delay at downstream airports is referred to as delay propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. (0.3 балла)** Какая из причин отмены рейса (`CancellationCode`) была самой частой? (расшифровки кодов можно найти в описании данных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent reason for flight cancellation in January-April 2008 was: A\n"
     ]
    }
   ],
   "source": [
    "# Filter the data for January to April 2008\n",
    "filtered_df = df[(df['Year'] == 2008) & (df['Month'] >= 1) & (df['Month'] <= 4)]\n",
    "\n",
    "# Count the occurrences of each CancellationCode\n",
    "cancellation_counts = filtered_df['CancellationCode'].value_counts()\n",
    "\n",
    "# Print the most frequent reason for cancellation\n",
    "most_frequent_reason = cancellation_counts.idxmax()\n",
    "print(\"The most frequent reason for flight cancellation in January-April 2008 was:\", most_frequent_reason)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. (0.3 балла)** Найдите среднее, минимальное и максимальное расстояние, пройденное самолетом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum distance traveled by an aircraft in January-April 2008 was: 31 miles\n"
     ]
    }
   ],
   "source": [
    "# Filter the data for January to April 2008\n",
    "filtered_df = df[(df['Year'] == 2008) & (df['Month'] >= 1) & (df['Month'] <= 4)]\n",
    "\n",
    "# Find the minimum distance traveled\n",
    "min_distance = filtered_df['Distance'].min()\n",
    "\n",
    "print(\"The minimum distance traveled by an aircraft in January-April 2008 was:\", min_distance, \"miles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average distance traveled by aircraft in January-April 2008 was: 722.2696525356077 miles\n"
     ]
    }
   ],
   "source": [
    "# Filter the data for January to April 2008\n",
    "filtered_df = df[(df['Year'] == 2008) & (df['Month'] >= 1) & (df['Month'] <= 4)]\n",
    "\n",
    "# Calculate the average distance traveled\n",
    "average_distance = filtered_df['Distance'].mean()\n",
    "\n",
    "print(\"The average distance traveled by aircraft in January-April 2008 was:\", average_distance, \"miles\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum distance traveled by an aircraft in January-April 2008 was: 4962 miles\n"
     ]
    }
   ],
   "source": [
    "# Filter the data for January to April 2008\n",
    "filtered_df = df[(df['Year'] == 2008) & (df['Month'] >= 1) & (df['Month'] <= 4)]\n",
    "\n",
    "# Find the maximum distance traveled\n",
    "max_distance = filtered_df['Distance'].max()\n",
    "\n",
    "print(\"The maximum distance traveled by an aircraft in January-April 2008 was:\", max_distance, \"miles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average distance traveled by aircraft in January-April 2008 was: 722.2696525356077 miles\n",
      "The minimum distance traveled by an aircraft in January-April 2008 was: 31 miles\n",
      "The maximum distance traveled by an aircraft in January-April 2008 was: 4962 miles\n"
     ]
    }
   ],
   "source": [
    "# Filter the data for January to April 2008\n",
    "filtered_df = df[(df['Year'] == 2008) & (df['Month'] >= 1) & (df['Month'] <= 4)]\n",
    "\n",
    "# Calculate the average distance traveled\n",
    "average_distance = filtered_df['Distance'].mean()\n",
    "\n",
    "# Find the minimum distance traveled\n",
    "min_distance = filtered_df['Distance'].min()\n",
    "\n",
    "# Find the maximum distance traveled\n",
    "max_distance = filtered_df['Distance'].max()\n",
    "\n",
    "print(\"The average distance traveled by aircraft in January-April 2008 was:\", average_distance, \"miles\")\n",
    "print(\"The minimum distance traveled by an aircraft in January-April 2008 was:\", min_distance, \"miles\")\n",
    "print(\"The maximum distance traveled by an aircraft in January-April 2008 was:\", max_distance, \"miles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. (0.3 балла)** Не выглядит ли подозрительным минимальное пройденное расстояние? В какие дни и на каких рейсах оно было? Какое расстояние было пройдено этими же рейсами в другие дни?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum distance traveled by an aircraft in January-April 2008 was: 31 miles\n",
      "\n",
      "Flights with minimum distance:\n",
      "       Year  Month  DayofMonth  FlightNum  Distance\n",
      "27534  2008      3          11         64        31\n",
      "48112  2008      2          28         64        31\n",
      "\n",
      "Distances covered by the same flights on other days:\n",
      "       Year  Month  DayofMonth  FlightNum  Distance\n",
      "27534  2008      3          11         64        31\n",
      "48112  2008      2          28         64        31\n"
     ]
    }
   ],
   "source": [
    "# Filter the data for January to April 2008\n",
    "filtered_df = df[(df['Year'] == 2008) & (df['Month'] >= 1) & (df['Month'] <= 4)]\n",
    "\n",
    "# Find the minimum distance traveled\n",
    "min_distance = filtered_df['Distance'].min()\n",
    "\n",
    "# Find the flights and days associated with the minimum distance\n",
    "min_distance_flights = filtered_df[filtered_df['Distance'] == min_distance]\n",
    "\n",
    "# Find the distances covered by the same flights on other days\n",
    "similar_flights = filtered_df[filtered_df['FlightNum'].isin(min_distance_flights['FlightNum'])]\n",
    "similar_flights = similar_flights[similar_flights['DayofMonth'].isin(min_distance_flights['DayofMonth'])]\n",
    "\n",
    "# Print the results\n",
    "print(\"The minimum distance traveled by an aircraft in January-April 2008 was:\", min_distance, \"miles\")\n",
    "\n",
    "print(\"\\nFlights with minimum distance:\")\n",
    "print(min_distance_flights[['Year', 'Month', 'DayofMonth', 'FlightNum', 'Distance']])\n",
    "\n",
    "print(\"\\nDistances covered by the same flights on other days:\")\n",
    "print(similar_flights[['Year', 'Month', 'DayofMonth', 'FlightNum', 'Distance']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. (0.3 балла)** Из какого аэропорта было произведено больше всего вылетов? В каком городе он находится?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The airport with the most departures in January-April 2008 was: ATL\n"
     ]
    }
   ],
   "source": [
    "# Filter the data for January to April 2008\n",
    "filtered_df = df[(df['Year'] == 2008) & (df['Month'] >= 1) & (df['Month'] <= 4)]\n",
    "\n",
    "# Find the airport with the most departures\n",
    "most_departures_airport = filtered_df['Origin'].value_counts().idxmax()\n",
    "\n",
    "print(\"The airport with the most departures in January-April 2008 was:\", most_departures_airport)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. (0.3 балла)** Найдите для каждого аэропорта среднее время полета (`AirTime`) по всем вылетевшим из него рейсам. Какой аэропорт имеет наибольшее значение этого показателя?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аэропорт с наибольшим средним временем полета: SJU\n",
      "Наибольшее среднее время полета: 205.2\n"
     ]
    }
   ],
   "source": [
    "# Создайте группировку по аэропорту и вычислите среднее значение AirTime для каждой группы\n",
    "airport_avg_airtime = df.groupby('Origin')['AirTime'].mean()\n",
    "\n",
    "# Найдите аэропорт с наибольшим средним временем полета\n",
    "airport_with_max_avg_airtime = airport_avg_airtime.idxmax()\n",
    "max_avg_airtime_value = airport_avg_airtime.max()\n",
    "\n",
    "print(\"Аэропорт с наибольшим средним временем полета:\", airport_with_max_avg_airtime)\n",
    "print(\"Наибольшее среднее время полета:\", max_avg_airtime_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. (0.5 балла)** Найдите аэропорт, у которого наибольшая доля задержанных (`DepDelay > 0`) рейсов. Исключите при этом из рассмотрения аэропорты, из которых было отправлено меньше 1000 рейсов (используйте функцию `filter` после `groupby`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аэропорт с наибольшей долей задержанных рейсов: EWR\n",
      "Наибольшая доля задержанных рейсов: 0.5111591072714183\n"
     ]
    }
   ],
   "source": [
    "# Создайте группировку по аэропорту вылета (Origin)\n",
    "airport_groups = df.groupby('Origin')\n",
    "\n",
    "# Функция для фильтрации аэропортов с менее чем 1000 рейсов\n",
    "def filter_airports(group):\n",
    "    return len(group) >= 1000\n",
    "\n",
    "# Отфильтруйте аэропорты с менее чем 1000 рейсов\n",
    "filtered_airports = airport_groups.filter(filter_airports)\n",
    "\n",
    "# Сгруппируйте отфильтрованные данные по аэропорту и вычислите долю задержанных рейсов\n",
    "delayed_flight_percentage = filtered_airports.groupby('Origin')['DepDelay'].apply(lambda x: (x > 0).mean())\n",
    "\n",
    "# Найдите аэропорт с наибольшей долей задержанных рейсов\n",
    "airport_with_max_delayed_percentage = delayed_flight_percentage.idxmax()\n",
    "max_delayed_percentage_value = delayed_flight_percentage.max()\n",
    "\n",
    "print(\"Аэропорт с наибольшей долей задержанных рейсов:\", airport_with_max_delayed_percentage)\n",
    "print(\"Наибольшая доля задержанных рейсов:\", max_delayed_percentage_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия\n",
    "\n",
    "В этой части мы разберемся с линейной регрессией, способами её обучения и измерением качества ее прогнозов. \n",
    "\n",
    "Будем рассматривать датасет из предыдущей части задания для предсказания времени задержки отправления рейса в минутах (DepDelay). Отметим, что под задержкой подразумевается не только опоздание рейса относительно планируемого времени вылета, но и отправление до планируемого времени.\n",
    "\n",
    "### Подготовка данных\n",
    "\n",
    "**12. (0.5 балла)** Считайте выборку из файла при помощи функции pd.read_csv и ответьте на следующие вопросы:\n",
    "   - Имеются ли в данных пропущенные значения?\n",
    "   - Сколько всего пропущенных элементов в таблице \"объект-признак\"?\n",
    "   - Сколько объектов имеют хотя бы один пропуск?\n",
    "   - Сколько признаков имеют хотя бы одно пропущенное значение?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there any missing values in the data? True\n",
      "Total number of missing values in the 'object-feature' table: 355215\n",
      "Number of objects with at least one missing value: 70000\n",
      "Number of features with at least one missing value: 16\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the dataset\n",
    "has_missing_values = df.isnull().any().any()  # True if there are any missing values, False otherwise\n",
    "\n",
    "# Count the total number of missing values in the \"object-feature\" table\n",
    "total_missing_values = df.isnull().sum().sum()\n",
    "\n",
    "# Count the number of objects (rows) that have at least one missing value\n",
    "objects_with_missing_values = df[df.isnull().any(axis=1)].shape[0]\n",
    "\n",
    "# Count the number of features (columns) that have at least one missing value\n",
    "features_with_missing_values = df.columns[df.isnull().any()].shape[0]\n",
    "\n",
    "# Print the results\n",
    "print(\"Are there any missing values in the data?\", has_missing_values)\n",
    "print(\"Total number of missing values in the 'object-feature' table:\", total_missing_values)\n",
    "print(\"Number of objects with at least one missing value:\", objects_with_missing_values)\n",
    "print(\"Number of features with at least one missing value:\", features_with_missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы понимаете, также не имеет смысла рассматривать при решении поставленной задачи объекты с пропущенным значением целевой переменной. В связи с этим ответьте на следующие вопросы и выполните соответствующие действия:\n",
    "- Имеются ли пропущенные значения в целевой переменной?\n",
    "- Проанализируйте объекты с пропущенными значениями целевой переменной. Чем вызвано это явление? Что их объединяет? Можно ли в связи с этим, на ваш взгляд, исключить какие-то признаки из рассмотрения? Обоснуйте свою точку зрения.\n",
    "\n",
    "Исключите из выборки объекты **с пропущенным значением целевой переменной и со значением целевой переменной, равным 0**, а также при необходимости исключите признаки в соответствии с вашим ответом на последний вопрос из списка и выделите целевую переменную в отдельный вектор, исключив её из матрицы \"объект-признак\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (319367099.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[40], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    if 2008.csv[\"DepDelay\"].isnull().any():\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "if 2008.csv[\"DepDelay\"].isnull().any():\n",
    "    print(\"Есть пропуски в целевой переменной\")\n",
    "else:\n",
    "    print(\"Нет пропусков в целевой переменной\")\n",
    "    \n",
    "temp = 2008.csv[2008.csv[\"DepDelay\"].isnull() == True]\n",
    "pd.unique(temp[\"Cancelled\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13. (0.5 балла)** Обратите внимание, что признаки DepTime, CRSDepTime, ArrTime, CRSArrTime приведены в формате hhmm, в связи с чем будет не вполне корректно рассматривать их как вещественные.\n",
    "\n",
    "Преобразуйте каждый признак FeatureName из указанных в пару новых признаков FeatureName\\_Hour, FeatureName\\_Minute, разделив каждое из значений на часы и минуты. Не забудьте при этом исключить исходный признак из выборки. В случае, если значение признака отсутствует, значения двух новых признаков, его заменяющих, также должны отсутствовать. \n",
    "\n",
    "Например, признак DepTime необходимо заменить на пару признаков DepTime_Hour, DepTime_Minute. При этом, например, значение 155 исходного признака будет преобразовано в значения 1 и 55 признаков DepTime_Hour, DepTime_Minute соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import pandas as pd\n",
    "\n",
    "# Загрузите данные из файла \"2008.csv\" (убедитесь, что файл находится в рабочем каталоге или укажите полный путь)\n",
    "df = pd.read_csv(\"2008.csv\")\n",
    "\n",
    "# Список признаков, которые нужно преобразовать\n",
    "features_to_transform = ['DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime']\n",
    "\n",
    "# Пройдите по каждому признаку и создайте новые признаки FeatureName_Hour и FeatureName_Minute\n",
    "for feature in features_to_transform:\n",
    "    # Создайте новые признаки с нулевыми значениями\n",
    "    df[f'{feature}_Hour'] = 0\n",
    "    df[f'{feature}_Minute'] = 0\n",
    "    \n",
    "    # Ищите ненулевые значения и обработайте их\n",
    "    not_null_indices = df[df[feature].notnull()].index\n",
    "    df.loc[not_null_indices, f'{feature}_Hour'] = df.loc[not_null_indices, feature] // 100\n",
    "    df.loc[not_null_indices, f'{feature}_Minute'] = df.loc[not_null_indices, feature] % 100\n",
    "    \n",
    "# Удалите исходные признаки\n",
    "df = df.drop(columns=features_to_transform)\n",
    "\n",
    "# Теперь df содержит новые признаки FeatureName_Hour и FeatureName_Minute для указанных признаков,\n",
    "# исходные признаки были удалены.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14. (0.5 балла)** Некоторые из признаков, отличных от целевой переменной, могут оказывать чересчур значимое влияние на прогноз, поскольку по своему смыслу содержат большую долю информации о значении целевой переменной. Изучите описание датасета и исключите признаки, сильно коррелирующие с ответами. Ваш выбор признаков для исключения из выборки обоснуйте. Кроме того, исключите признаки TailNum и Year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15. (1 балл)** Приведем данные к виду, пригодному для обучения линейных моделей. Для этого вещественные признаки надо отмасштабировать, а категориальные — привести к числовому виду. Также надо устранить пропуски в данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первую очередь поймем, зачем необходимо применять масштабирование. Следующие ячейки с кодом построят гистограммы для 3 вещественных признаков выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X['DepTime_Hour'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['TaxiIn'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['FlightNum'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какую проблему вы наблюдаете на этих графиках? Как масштабирование поможет её исправить?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые из признаков в нашем датасете являются категориальными. Типичным подходом к работе с ними является бинарное, или [one-hot-кодирование](https://en.wikipedia.org/wiki/One-hot).\n",
    "\n",
    "Реализуйте функцию transform_data, которая принимает на вход DataFrame с признаками и выполняет следующие шаги:\n",
    "1. Замена пропущенных значений на нули для вещественных признаков и на строки 'nan' для категориальных.\n",
    "2. Масштабирование вещественных признаков с помощью [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "3. One-hot-кодирование категориальных признаков с помощью [DictVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) или функции [pd.get_dummies](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html).\n",
    "\n",
    "Метод должен возвращать преобразованный DataFrame, который должна состоять из масштабированных вещественных признаков и закодированных категориальных (исходные признаки должны быть исключены из выборки)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_data(data):\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примените функцию transform_data к данным. Сколько признаков получилось после преобразования?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**16. (0.5 балла)** Разбейте выборку и вектор целевой переменной на обучение и контроль в отношении 70/30 (для этого можно использовать, например, функцию [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn\n",
    "\n",
    "<img src = \"https://pp.vk.me/c4534/u35727827/93547647/x_d31c4463.jpg\">\n",
    "Теперь, когда мы привели данные к пригодному виду, попробуем решить задачу при помощи метода наименьших квадратов. Напомним, что данный метод заключается в оптимизации функционала $MSE$:\n",
    "\n",
    "$$MSE(X, y) = \\frac{1}{l} \\sum_{i=1}^l (<w, x_i> - y_i)^2 \\to \\min_{w},$$\n",
    "\n",
    "где $\\{ (x_i, y_i ) \\}_{i=1}^l$ — обучающая выборка, состоящая из $l$ пар объект-ответ.\n",
    "\n",
    "Заметим, что решение данной задачи уже реализовано в модуле sklearn в виде класса [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression).\n",
    "\n",
    "**17. (0.5 балла)** Обучите линейную регрессию на 1000 объектах из обучающей выборки и выведите значения $MSE$ и $R^2$ на этой подвыборке и контрольной выборке (итого 4 различных числа). Проинтерпретируйте полученный результат — насколько качественные прогнозы строит полученная модель? Какие проблемы наблюдаются в модели?\n",
    "\n",
    "**Подсказка**: изучите значения полученных коэффициентов $w$, сохраненных в атрибуте coef_ объекта LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'EV'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Обучаем модель на подвыборке\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_subsample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_subsample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Делаем прогнозы на подвыборке и контрольной выборке\u001b[39;00m\n\u001b[0;32m     15\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train_subsample)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:648\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    644\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    646\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 648\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    652\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(\n\u001b[0;32m    653\u001b[0m     sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype, only_non_negative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    654\u001b[0m )\n\u001b[0;32m    656\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[38;5;241m=\u001b[39m _preprocess_data(\n\u001b[0;32m    657\u001b[0m     X,\n\u001b[0;32m    658\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    661\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    662\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2070\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'EV'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Выберем подвыборку из обучающей выборки из 1000 объектов\n",
    "X_train_subsample = X_train[:1000]\n",
    "y_train_subsample = y_train[:1000]\n",
    "\n",
    "# Создаем объект модели линейной регрессии\n",
    "model = LinearRegression()\n",
    "\n",
    "# Обучаем модель на подвыборке\n",
    "model.fit(X_train_subsample, y_train_subsample)\n",
    "\n",
    "# Делаем прогнозы на подвыборке и контрольной выборке\n",
    "y_train_pred = model.predict(X_train_subsample)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Вычисляем MSE и R^2 на подвыборке\n",
    "mse_train = mean_squared_error(y_train_subsample, y_train_pred)\n",
    "r2_train = r2_score(y_train_subsample, y_train_pred)\n",
    "\n",
    "# Вычисляем MSE и R^2 на контрольной выборке\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Выводим результаты\n",
    "print(f\"Train MSE: {mse_train:.2f}\")\n",
    "print(f\"Train R^2: {r2_train:.2f}\")\n",
    "print(f\"Test MSE: {mse_test:.2f}\")\n",
    "print(f\"Test R^2: {r2_test:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Для решения описанных вами в предыдущем пункте проблем используем L1- или L2-регуляризацию, тем самым получив Lasso и Ridge регрессии соответственно и изменив оптимизационную задачу одним из следующих образов:\n",
    "$$MSE_{L1}(X, y) = \\frac{1}{l} \\sum_{i=1}^l (<w, x_i> - y_i)^2 + \\alpha ||w||_1 \\to \\min_{w},$$\n",
    "$$MSE_{L2}(X, y) = \\frac{1}{l} \\sum_{i=1}^l (<w, x_i> - y_i)^2 + \\alpha ||w||_2^2 \\to \\min_{w},$$\n",
    "\n",
    "где $\\alpha$ — коэффициент регуляризации. Один из способов его подбора заключается в переборе некоторого количества значений и оценке качества на кросс-валидации для каждого из них, после чего выбирается значение, для которого было получено наилучшее качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__18. (1 балл) __ Обучение линейной регрессии.\n",
    "\n",
    "\n",
    "\n",
    "Обучите линейную регрессию с $L_1$ (Lasso) и $L_2$ (Ridge) регуляризаторами (используйте параметры по умолчанию). Посмотрите, какое количество коэффициентов близко к 0 (степень близости к 0 определите сами из разумных пределов). Постройте график зависимости числа ненулевых коэффициентов от коэффицента регуляризации (перебирайте значения по логарифмической сетке от $10^{-3}$ до $10^3$). Согласуются ли результаты с вашими ожиданиями?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитайте для Ridge-регрессии следующие метрики: $RMSE$, $MAE$, $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберите на обучающей выборке для Ridge-регрессии коэффициент регуляризации (перебирайте значения по логарифмической сетке от $10^{-3}$ до $10^3$) для каждой из метрик при помощи кросс-валидации c 5 фолдами на тех же 1000 объектах. Для этого воспользуйтесь GridSearchCV и KFold из sklearn. Постройте графики зависимости фукнции потерь от коэффициента регуляризации. Посчитайте те же метрики снова. Заметно ли изменилось качество?\n",
    "\n",
    "Для выполнения данного задания вам могут понадобиться реализованные в библиотеке объекты [LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html), [RidgeCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) и [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__19. (0.5 балла)__ Поиск объектов-выбросов\n",
    "\n",
    "\n",
    "Как известно, MSE сильно штрафует за большие ошибки на объектах-выбросах. С помощью cross_val_predict сделайте Out-of-Fold предсказания для обучающей выборки. Посчитайте ошибки и посмотрите на их распределение (plt.hist). Что вы видите?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
